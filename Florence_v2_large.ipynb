{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOMcMQ5TsWDCKZtaPv8mByx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tushar-Nagar-64/GW_echoes/blob/master/Florence_v2_large.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up functions to run Florence"
      ],
      "metadata": {
        "id": "a6emE-8s09_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers requests torch pillow\n",
        "!pip install timm flash_attn"
      ],
      "metadata": {
        "id": "LW-SGeW8tB6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from PIL import Image\n",
        "import requests\n",
        "import copy\n",
        "import torch\n",
        "%matplotlib inline\n",
        "\n",
        "model_id = 'microsoft/Florence-2-large'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().cuda()\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h3cLRO5QtGTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_example(image, task_prompt, text_input=None):\n",
        "    if text_input is None:\n",
        "        prompt = task_prompt\n",
        "    else:\n",
        "        prompt = task_prompt + text_input\n",
        "\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to('cuda', torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "      input_ids=inputs[\"input_ids\"].cuda(),\n",
        "      pixel_values=inputs[\"pixel_values\"].cuda(),\n",
        "      max_new_tokens=1024,\n",
        "      early_stopping=False,\n",
        "      do_sample=False,\n",
        "      num_beams=3,\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "    parsed_answer = processor.post_process_generation(\n",
        "        generated_text,\n",
        "        task=task_prompt,\n",
        "        image_size=(image.width, image.height)\n",
        "    )\n",
        "\n",
        "    return parsed_answer"
      ],
      "metadata": {
        "id": "qEI6z3xT2m9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_od_format(data):\n",
        "    \"\"\"\n",
        "    Converts a dictionary with 'bboxes' and 'bboxes_labels' into a dictionary with separate 'bboxes' and 'labels' keys.\n",
        "\n",
        "    Parameters:\n",
        "    - data: The input dictionary with 'bboxes', 'bboxes_labels', 'polygons', and 'polygons_labels' keys.\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary with 'bboxes' and 'labels' keys formatted for object detection results.\n",
        "    \"\"\"\n",
        "    # Extract bounding boxes and labels\n",
        "    bboxes = data.get('bboxes', [])\n",
        "    labels = data.get('bboxes_labels', [])\n",
        "\n",
        "    # Construct the output format\n",
        "    od_results = {\n",
        "        'bboxes': bboxes,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "    return od_results"
      ],
      "metadata": {
        "id": "lalCQl_NxODM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "def plot_bbox(image, data):\n",
        "   # Create a figure and axes\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(image)\n",
        "\n",
        "    # Plot each bounding box\n",
        "    for bbox, label in zip(data['bboxes'], data['labels']):\n",
        "        # Unpack the bounding box coordinates\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        # Add the rectangle to the Axes\n",
        "        ax.add_patch(rect)\n",
        "        # Annotate the label\n",
        "        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "    # Remove the axis ticks and labels\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wpw6yUJ4Fl5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def extract_faces(image, bboxes):\n",
        "    faces = []\n",
        "    for bbox in bboxes:\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        # Crop the face from the image\n",
        "        face = np.array(image)[int(y1):int(y2), int(x1):int(x2)]\n",
        "        faces.append(face)\n",
        "    return faces\n"
      ],
      "metadata": {
        "id": "q__rQrgfPHOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def preprocess_faces(faces, size=(160, 160)):\n",
        "    \"\"\"\n",
        "    Preprocess a list of face images: convert to PIL, resize, and normalize.\n",
        "\n",
        "    Parameters:\n",
        "    - faces (list of numpy.ndarray): List of face images in BGR format.\n",
        "    - size (tuple): Desired output size (width, height) for resizing.\n",
        "\n",
        "    Returns:\n",
        "    - List of preprocessed face images as numpy arrays with shape (1, height, width, channels).\n",
        "    \"\"\"\n",
        "    processed_faces = []\n",
        "\n",
        "    for face in faces:\n",
        "        # Convert BGR numpy array to RGB numpy array\n",
        "        # rgb_image = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "        rgb_image = face\n",
        "\n",
        "        # Convert RGB numpy array to PIL image\n",
        "        pil_image = Image.fromarray(rgb_image)\n",
        "\n",
        "        # Resize image\n",
        "        pil_image = pil_image.resize(size)\n",
        "\n",
        "        # Convert PIL Image to numpy array and normalize\n",
        "        face_array = np.array(pil_image) / 255.0\n",
        "\n",
        "        # Add a batch dimension\n",
        "        face_array = np.expand_dims(face_array, axis=0)\n",
        "\n",
        "        processed_faces.append(face_array)\n",
        "\n",
        "    return processed_faces\n"
      ],
      "metadata": {
        "id": "cg_tDiNBOtFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Florence"
      ],
      "metadata": {
        "id": "X4rn2m8X1BM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(url, task_prompt:str='<OPEN_VOCABULARY_DETECTION>', search_term:str='human head'):\n",
        "    image_bits = Image.open(requests.get(url, stream=True).raw)\n",
        "    results = run_example(image_bits, task_prompt, text_input=\"human head\")\n",
        "    bbox_results  = convert_to_od_format(results[task_prompt])\n",
        "\n",
        "    image = image_bits\n",
        "\n",
        "    # plot_bbox(image, bbox_results)\n",
        "    faces = extract_faces(image, bbox_results['bboxes'])\n",
        "    processed_faces = preprocess_faces(extract_faces(image, bbox_results['bboxes']))\n",
        "\n",
        "    return(bbox_results, faces, processed_faces)"
      ],
      "metadata": {
        "id": "kyrcmXf62tm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[bbox,faces,processed_faces] = run(url=\"https://th.bing.com/th/id/OIP.kjOAjQunGxvlajiw77c_XQHaE8?rs=1&pid=ImgDetMain\")"
      ],
      "metadata": {
        "id": "7KhgXTA1HrUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FaceNet embeddings"
      ],
      "metadata": {
        "id": "UTa1taXwTzkx"
      }
    }
  ]
}